# Word Embedding Intuition with Tensorflow 2.0

This is a simple demonstration of how word embedding works in tensorflow 2.0. The implementation is directly based on this tutorial paper:
- [word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738)
- and its online demo [wevi](https://ronxin.github.io/wevi/)

which are based on these two pioneering works:
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)

The [wevi](https://ronxin.github.io/wevi/) demo has already given an amazing visualization of word embedding. This project tries to replicate its result using Tensorflow 2.x, which is more practical, and whose code can be reused in the future work.

# Environment
The code is tested under these packages:
- Python 3.6
- numpy 1.17.2
- sklearn 0.21.3
- matplotlib 3.1.1
- tensorflow 2.0.0

# Colab Version
Here is an online [Colab notebook](https://drive.google.com/open?id=1CAYHU-6GkTD1BTnIF2yHs2_N_rrKYoEn) for a quick look, although it's quite bulky.
